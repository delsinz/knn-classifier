%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University Assignment Title Page 
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
% Instructions for using this template:
% This title page is capable of being compiled as is. This is not useful for 
% including it in another document. To do this, you have two options: 
%
% 1) Copy/paste everything between \begin{document} and \end{document} 
% starting at \begin{titlepage} and paste this into another LaTeX file where you 
% want your title page.
% OR
% 2) Remove everything outside the \begin{titlepage} and \end{titlepage} and 
% move this file to the same directory as the LaTeX file you wish to add it to. 
% Then add \input{./title_page_1.tex} to your LaTeX file where you want your
% title page.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{Title page with logo}
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{geometry}
\pagestyle{fancy}
\geometry
{
 a4paper,==
 total={170mm,257mm},
 left=20mm,
 top=20mm,
}
\fancyheadoffset{0.1cm}
\lhead{Mingyang Zhang : 65024, Shreyash Patodia : 767336}
\rhead{mingyang, spatodia}



\begin{document}

\section{Introduction}

DISTANCE MEASURE AND VOTING METHOD FOR EVERYTHING AND DATASET IN QUESTION. (Use all metrics for all things). 

In this assignment we were asked to classify instances in the Abalone Dataset into one of two 
('young' or 'old') or three ('very young', 'old' and 'middle-age') categories. The classification method meant to be used was K-Nearest Neighbours. 


\section{Similarity Metrics}

\subsection{Euclidean Distance, Manhattan Distance and Minkowski Distance}

The Abalone data set contains eight attributes out of which 7 are of a continuous nature, so it made sense to us to use some sort of distance metric to evaluate similarity instead of checking for equality (like in the case for Jaccard and Dice Similarity) or doing something like cosine similarity (which would compare the angles between the abalones since two abalones can be very similar in proportions, and thus have small angle and high cosine similarity but one may be much larger and older than the other). If p and q are the instances to be compared the formula for our similarity metrics are: \\

Euclidean Distance:
\begin{equation}
\sqrt{\sum_{i=1}^{n} (p_{i} - q_{i})^{2}}
\end{equation}

Manhattan Distance:
\begin{equation}
\sum_{i=1}^{n} |p_{i} - q_{i}|
\end{equation}

Minkowski Distance:
\begin{equation}
{(\sum_{i=1}^{n} {|p_{i} - q_{i}|}^{pow})}^{1/pow}
\end{equation}

(We take pow = 0.5 for Minkowski distance, as it lets us makes for a nice comparison with Euclidean since Euclidean is basically Minkowski with p = 2) \\

Euclidean takes sum of squares before finding the root, Manhattan is plain addition of distances values and Minkowski (with p = 0.5) takes the root of each value before summing them up and squaring the sum. We especially wanted to contrast Minkowski and Euclidean since Minkowski is actually going to amplify larger distances (since most distances are < 1 and root of a number with value < 1 will be large than the number itself) and Euclidean would tend to supress it by squaring (square of number < 1 is less than the number itself). 


\subsection{Experimentation with similarity metrics}

Here are some results we got from testing our similarity metrics while varying other parameters in the program. 
The experiment shows us that that the performance of all of the similarity metrics is very closely lumped together, this is probably due to the fact that the distances are so small that it doesn't make much of a difference if we square or find the root of the values. 

(Note: All measurements in the tables are averages of 10 runs and 10 fold cross validation was used for each run)

\begin{tabular}{|p{2cm}|p{3cm}||p{2cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|     }
 \hline
 \multicolumn{6}{|c|}{Similarity Experiment} \\
 \hline
 Classification Type & Parameters & Similarity Metric & Accuracy & Precision & Recall\\
 \hline
 Abalone-3 & k=29,voting=inv. distance &   euclidean & 0.7676  & 0.6469 & 0.6487\\
 \hline
 Abalone-3 & k=29,voting=inv. distance &   minkowski & 0.7663 & 0.6468 & 0.6473\\
 \hline
 Abalone-3 & k=29,voting=inv. distance &   manhattan & 0.7641 & 0.6420 & 0.6436\\ 
 \hline
 \hline
 Abalone-2 & k=29,voting=inv. distance &   euclidean & 0.7857  & 0.7714 & 0.7399\\
 \hline
 Abalone-2 & k=29,voting=inv. distance &   minkowski & 0.7787 & 0.7654 & 0.7299\\
 \hline
 Abalone-2 & k=29,voting=inv. distance &   manhattan & 0.7830  & 0.7683 & 0.7355\\ 
 \hline
 
\end{tabular}


% Commands to include a figure:
%\begin{figure}[!htb]
%\centering
% \includegraphics[width=0.5\textwidth]{BadExample.png}
% \caption{\label{fig:frog}Bad Design Example}
%\end{figure}


\section{Validation Framework}

\subsection{M-Fold Cross Validation}

For our validation framework we used 10-Fold Cross Validation, we first divide the dataset into 10 (approximately) equal partitions and then perform K-Nearest Neighbour classification by choosing each of the 10 partitions as our test set (and the amalgamation of the remaining 9 as our training set) for one run of the K-Nearest Neighbour algorithm. This leads to very stable evaluation metrics across different runs of the program since performing the algorithm 10 times means that any positive outlier is averaged out by negative ones.

We also tested holdout, 5-Fold Cross Validation and 20-Fold Cross Validation and here is a graph of their accuracies for 10 different runs with abalone-2 and abalone-3 each:


% Commands to include a figure:
%\begin{figure}[!htb]
%\centering
% \includegraphics[width=0.5\textwidth]{BadExample.png}
% \caption{\label{fig:frog}Bad Design Example}
%\end{figure}



\section{Our choices}

\subsection{Representation of Data}

We are using a 2-tuple to represent the data set wherein the first element of the tuple is the list of instances and the second element of the data set is the list of class labels corresponding to the instances. 
In our pre-processing of the data we determined that it was best to work with all of the attributes since

\subsection{Some other remarks}



\end{document}